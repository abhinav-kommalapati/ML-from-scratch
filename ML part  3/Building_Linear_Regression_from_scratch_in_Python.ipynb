{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6Im0qJqweQC/qF1rp9kt3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Linear Regression:\n","\n","Y = wX + b\n","\n","Y --> Dependent Variable\n","\n","X --> Independent Variable\n","\n","w --> weight\n","\n","b --> bias\n","\n","\n","\n","Gradient Descent:\n","\n","Gradient Descent is an optimization algorithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.\n","\n","w = w - α*dw\n","\n","b = b - α*db\n","\n","\n","\n","Learning Rate:\n","\n","Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.\n","\n","\n","\n"],"metadata":{"id":"Zb0PtHm7nY_I"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"b6UCpXfFme5x","executionInfo":{"status":"ok","timestamp":1749538617100,"user_tz":-330,"elapsed":19,"user":{"displayName":"Abhinav Kommalapati","userId":"15700893586599230600"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","source":["Linear Regression"],"metadata":{"id":"D1STn-4KoMzQ"}},{"cell_type":"code","source":["class linear_regression():\n","\n","  def __init__(self, learning_rate, no_of_iterations):\n","    self.learning_rate = learning_rate\n","    self.no_of_iterations = no_of_iterations\n","\n","\n","  def fit(self,x, y): # x -> no of years experience , y -> salary\n","\n","    # number of training examples & number of features(years of eperience)\n","\n","    self.m, self.n = x.shape\n","\n","    #initiating the weight and bias\n","\n","    self.weight = np.zeros(self.n)\n","    self.b = 0\n","    self.x = x\n","    self.y = y\n","\n","   #implementing gradient descent\n","\n","  for i in range( self.no_of_iterations):\n","    self.update_weights()\n","\n","\n","  def update_weights(self):\n","\n","    y_prediction = self.predict(self.x)\n","\n","    #calculating the gradients\n","\n","    dw  = - (2 * (self.x.t).dot(self.y - y_prediction)) / self.m\n","    db = -2 * np.sum(self.y - y_prediction)/self.m\n","\n","    #updating the weights\n","\n","    self.w = self.w - self.learning_rate*dw\n","    self.b = self.b - self.learning_rate*db\n","\n","  def predict(self,x):\n","\n","    return x.dot(self.w) + self.b # y = wx + b\n",""],"metadata":{"id":"3yaS6hCroLV9"},"execution_count":null,"outputs":[]}]}